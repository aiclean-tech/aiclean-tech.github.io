{"version":3,"file":"static/js/975.9f6d5bd4.chunk.js","mappings":"kHAAO,MAAMA,EAAW,CACpB,CACIC,GAAI,GACJC,MAAO,wGACPC,KAAM,OACNC,KAAM,OACNC,SAAU,GACVC,OAAQ,kFACRC,KAAM,IAEV,CACIN,GAAI,GACJC,MAAO,oGACPC,KAAM,OACNC,KAAM,OACNC,SAAU,GACVC,OAAQ,4FACRC,KAAM,IAEV,CACIN,GAAI,GACJC,MAAO,wFACPC,KAAM,MACNC,KAAM,OACNC,SAAU,GACVC,OAAQ,kFACRC,KAAM,IAEV,CACIN,GAAI,GACJC,MAAO,qGACPC,KAAM,MACNC,KAAM,OACNC,SAAU,GACVC,OAAQ,+FACRC,KAAM,IAEV,CACIN,GAAI,GACJC,MAAO,uHACPC,KAAM,QACNC,KAAM,OACNC,SAAU,06DACVC,OAAQ,6CACRC,KAAM,4IAEV,CACIN,GAAI,GACJC,MAAO,0GACPC,KAAM,MACNC,KAAM,OACNC,SAAU,yuDACVC,OAAQ,yFACRC,KAAM,oCAEV,CACIN,GAAI,GACJC,MAAO,gFACPC,KAAM,MACNC,KAAM,OACNC,SAAU,k1EACVC,OAAQ,0CACRC,KAAM,0DAEV,CACIN,GAAI,GACJC,MAAO,wEACPC,KAAM,MACNC,KAAM,OACNC,SAAU,w/EACVC,OAAQ,+DACRC,KAAM,0CAEV,CACIN,GAAI,GACJC,MAAO,uEACPC,KAAM,OACNC,KAAM,OACNC,SAAU,+0CACVC,OAAQ,qEACRC,KAAM,mDAEV,CACIN,GAAI,GACJC,MAAO,4FACPC,KAAM,OACNC,KAAM,OACNC,SAAU,4iDACVC,OAAQ,uGACRC,KAAM,sDAEV,CACIN,GAAI,GACJC,MAAO,kHACPC,KAAM,OACNC,KAAM,OACNC,SAAU,q6DACVC,OAAQ,iEACRC,KAAM,kDAEV,CACIN,GAAI,GACJC,MAAO,oEACPC,KAAM,OACNC,KAAM,OACNC,SAAU,ynDACVC,OAAQ,0CACRC,KAAM,8GAEV,CACIN,GAAI,GACJC,MAAO,yEACPC,KAAM,UACNC,KAAM,OACNC,SAAU,y9CACVC,OAAQ,uDACRC,KAAM,gDAEV,CACIN,GAAI,GACJC,MAAO,sGACPC,KAAM,MACNC,KAAM,OACNC,SAAU,+lDACVC,OAAQ,0DACRC,KAAM,kDAEV,CACIN,GAAI,GACJC,MAAO,yDACPC,KAAM,YACNC,KAAM,OACNC,SAAU,qrCACVC,OAAQ,6CACRC,KAAM,uDAEV,CACIN,GAAI,GACJC,MAAO,wEACPC,KAAM,kBACNC,KAAM,OACNC,SAAU,muCACVC,OAAQ,8CACRC,KAAM,kDAEV,CACIN,GAAI,GACJC,MAAO,sFACPC,KAAM,eACNC,KAAM,OACNC,SAAU,+tBACVC,OAAQ,oFACRC,KAAM,wIAEV,CACIN,GAAI,GACJC,MAAO,4DACPC,KAAM,OACNC,KAAM,OACNC,SAAU,qkCACVC,OAAQ,+DACRC,KAAM,kDAEV,CACIN,GAAI,GACJC,MAAO,6EACPC,KAAM,0BACNC,KAAM,OACNC,SAAU,ygDACVC,OAAQ,yDACRC,KAAM,oCAEV,CACIN,GAAI,GACJC,MAAO,qEACPC,KAAM,SACNC,KAAM,OACNC,SAAU,+oCACVC,OAAQ,kEACRC,KAAM,0CAEV,CACIN,GAAI,GACJC,MAAO,sEACPC,KAAM,UACNC,KAAM,OACNC,SAAU,wkDACVC,OAAQ,2FACRC,KAAM,uDAEV,CACIN,GAAI,GACJC,MAAO,8DACPC,KAAM,OACNC,KAAM,OACNC,SAAU,wsDACVC,OAAQ,8CACRC,KAAM,wGAEV,CACIN,GAAI,GACJC,MAAO,6FACPC,KAAM,OACNC,KAAM,OACNC,SAAU,ylDACVC,OAAQ,0CACRC,KAAM,iDAEV,CACIN,GAAI,GACJC,MAAO,mEACPC,KAAM,OACNC,KAAM,OACNC,SAAU,60CACVC,OAAQ,2DACRC,KAAM,uDAEV,CACIN,GAAI,GACJC,MAAO,yDACPC,KAAM,OACNC,KAAM,OACNC,SAAU,goDACVC,OAAQ,8FACRC,KAAM,iDAEV,CACIN,GAAI,GACJC,MAAO,iGACPC,KAAM,kBACNC,KAAM,OACNC,SAAU,ojDACVC,OAAQ,4CACRC,KAAM,yFAEV,CACIN,GAAI,GACJC,MAAO,mDACPC,KAAM,kBACNC,KAAM,OACNC,SAAU,2iDACVC,OAAQ,iEACRC,KAAM,6EAEV,CACIN,GAAI,GACJC,MAAO,wFACPC,KAAM,MACNC,KAAM,OACNC,SAAU,6jDACVC,OAAQ,0CACRC,KAAM,0DAEV,CACIN,GAAI,GACJC,MAAO,kFACPC,KAAM,YACNC,KAAM,OACNC,SAAU,wrDACVC,OAAQ,qDACRC,KAAM,gDAEV,CACIN,GAAI,EACJC,MAAO,6DACPC,KAAM,QACNC,KAAM,OACNC,SAAU,y0DACVC,OAAQ,0CACRC,KAAM,sDAEV,CACIN,GAAI,EACJC,MAAO,kDACPC,KAAM,aACNC,KAAM,OACNC,SAAU,0lCACVC,OAAQ,mEACRC,KAAM,oEAEV,CACIN,GAAI,EACJC,MAAO,qGACPC,KAAM,OACNC,KAAM,OACNC,SAAU,o+DACVC,OAAQ,sFACRC,KAAM,sDAEV,CACIN,GAAI,EACJC,MAAO,kGACPC,KAAM,OACNC,KAAM,OACNC,SAAU,m0DACVC,OAAQ,+BACRC,KAAM,mDAEV,CACIN,GAAI,EACJC,MAAO,4EACPC,KAAM,mBACNC,KAAM,OACNC,SAAU,umDACVC,OAAQ,0CACRC,KAAM,iFAEV,CACIN,GAAI,EACJC,MAAO,6FACPC,KAAM,OACNC,KAAM,OACNC,SAAU,8iDACVC,OAAQ,iEACRC,KAAM,gDAEV,CACIN,GAAI,EACJC,MAAO,kFACPC,KAAM,OACNC,KAAM,OACNC,SAAU,2zCACVC,OAAQ,gDACRC,KAAM,+CAEV,CACIN,GAAI,EACJC,MAAO,qEACPC,KAAM,MACNC,KAAM,OACNC,SAAU,s0DACVC,OAAQ,8BACRC,KAAM,gDAEV,CACIN,GAAI,EACJC,MAAO,qFACPC,KAAM,QACNC,KAAM,OACNC,SAAU,4+BACVC,OAAQ,0CACRC,KAAM,6C","sources":["constants/articles.js"],"sourcesContent":["export const articles = [\r\n    {\r\n        id: 38,\r\n        title: \"Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.\",\r\n        conf: \"NDSS\",\r\n        date: \"2025\",\r\n        abstract: \"\",\r\n        author: \"Yanzuo Chen, Zhibo Liu, Yuanyuan Yuan, Sihang Hu, Tianxiang Li, and Shuai Wang.\",\r\n        link: \"\",\r\n    },\r\n    {\r\n        id: 37,\r\n        title: \"Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models.\",\r\n        conf: \"ICSE\",\r\n        date: \"2025\",\r\n        abstract: \"\",\r\n        author: \"Kunpeng Zhang, Shuai Wang, Jitao Han, Xiaogang Zhu, Xian Li, Shaohua Wang, and Sheng Wen.\",\r\n        link: \"\",\r\n    },\r\n    {\r\n        id: 36,\r\n        title: \"DeepCache: Revisiting Cache Side-Channel Attacks in Deep Neural Networks Executables.\",\r\n        conf: \"CCS\",\r\n        date: \"2024\",\r\n        abstract: \"\",\r\n        author: \"Zhibo Liu, Yuanyuan Yuan, Yanzuo Chen, Sihang Hu, Tianxiang Li, and Shuai Wang.\",\r\n        link: \"\",\r\n    },\r\n    {\r\n        id: 35,\r\n        title: \"HyperTheft: Thieving Model Weights from TEE-Shielded Neural Networks via Ciphertext Side Channels.\",\r\n        conf: \"CCS\",\r\n        date: \"2024\",\r\n        abstract: \"\",\r\n        author: \"Yuanyuan Yuan, Zhibo Liu, Sen Deng, Yanzuo Chen, Shuai Wang, Yinqian Zhang, and Zhendong Su.\",\r\n        link: \"\",\r\n    },\r\n    {\r\n        id: 34,\r\n        title: \"See the Forest, not Trees: Unveiling and Escaping the Pitfalls of Error-Triggering Inputs in Neural Network Testing.\",\r\n        conf: \"ISSTA\",\r\n        date: \"2024\",\r\n        abstract: \"Recent efforts in deep neural network (DNN) testing commonly use error-triggering inputs (ETIs) to quantify DNN errors and to finetune the tested DNN for repairing. This study reveals the pitfalls of ETIs in DNN testing. Specifically, merely seeking for more ETIs “traps” the testing campaign into local plateaus, where similar ETIs are continuously generated using a few fixed input transformations. Similarly, fine-tuning the DNN with ETIs, while capable of fixing the exposed DNN mis-predictions, undermines the DNN’s resilience towards certain input transformations. However, these ETI-induced pitfalls have been overlooked in previous research, due to the insufficient input transformations (usually < 10), and we show that the severity of such deceptive phenomena is enlarged when testing DNNs with more and diverse real-life input transformations. \\n This paper presents a comprehensive study on the pitfalls of ETIs in DNN testing. We first augment conventional DNN testing pipelines with a large set of input transformations; the correctness and validity of these new transformations are verified with largescale human studies. Based on this, we show that launching an endless pursuit for ETIs cannot alleviate the “trapped testing” issue, and the undermined resilience pervasively occurs in many input transformations. Accordingly, we propose a novel and holistic viewpoint over DNN errors: instead of counting which input triggers a DNN mis-prediction, we record which input transformation can generate ETIs. The targeted input property of this transformation, termed erroneous property (EP), counts one DNN error and guides DNN testing (i.e., our new paradigm aims to find more EPs rather than ETIs). Evaluation shows that this EP-oriented testing paradigm significantly expands the explored DNN error space. Moreover, finetuning DNNs with EPs effectively improves their resilience towards different input transformations.\",\r\n        author: \"Yuanyuan Yuan, Shuai Wang and Zhendong Su.\",\r\n        link: \"https://2024.issta.org/details/issta-2024-papers/128/See-the-Forest-not-Trees-Unveiling-and-Escaping-the-Pitfalls-of-Error-Triggering-In\",\r\n    },\r\n    {\r\n        id: 33,\r\n        title: \"Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior.\",\r\n        conf: \"KDD\",\r\n        date: \"2024\",\r\n        abstract: \"Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables). \\n The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery. \\n To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a ``point-estimation'', SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.\",\r\n        author: \"Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, and Dongmei Zhang.\",\r\n        link: \"https://arxiv.org/abs/2406.10537\",\r\n    },\r\n    {\r\n        id: 32,\r\n        title: \"Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing\",\r\n        conf: \"TSE\",\r\n        date: \"2024\",\r\n        abstract: \"Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat). To test DNNs, diverse inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. State-of-the-art (SOTA) works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually valid (e.g., a cat remains a “cat” after mutation), existing efforts rely on imprecise and less generalizable heuristics. This study revisits two key objectives in media input mutation — perception diversity ( Div ) and validity ( Val ) — in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that Div and Val inextricably bound each other, and prove that SOTA generative model-based methods fundamentally fail to mutate real-world media data (either sacrificing Div or Val ). In contrast, we discuss the feasibility of mutating real-world media data with provably high Div and Val based on manifold. Following, we concretize the technical solution of mutating media data of various formats (images, audios, text) via a unified manner based on manifold. Specifically, when media data are projected into a low-dimensional manifold, the data can be mutated by walking on the manifold with certain directions and step sizes. When contrasted with the input data, the mutated data exhibit encouraging Div in the perceptual traits (e.g., lying vs. standing dog) while retaining reasonably high Val (i.e., a dog remains a dog). We implement our techniques in DeepWalk for testing DNNs. DeepWalk constructs manifolds for media data offline. In online testing, DeepWalk walks on manifolds to generate mutated media data with provably high Div and Val . Our evaluation tests DNNs executing various tasks (e.g., classification, self-driving, machine translation) and media data of different types (image, audio, text). DeepWalk outperforms prior methods in terms of the testing comprehensiveness and can find more error-triggering inputs with higher quality. The tested DNNs, after repaired using DeepWalk 's findings, exhibit better accuracy.\",\r\n        author: \"Yuanyuan Yuan, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://ieeexplore.ieee.org/abstract/document/10462634\",\r\n    },\r\n    {\r\n        id: 31,\r\n        title: \"Metamorphic Testing of Secure Multi-party Computation (MPC) Compilers\",\r\n        conf: \"FSE\",\r\n        date: \"2024\",\r\n        abstract: \"The demanding need to perform privacy-preserving computations among multiple data owners has led to the prosperous development of secure multi-party computation (MPC) protocols. MPC offers protocols for parties to jointly compute a function over their inputs while keeping those inputs private. To date, MPC has been widely adopted in various real-world, privacy-sensitive sectors, such as healthcare and finance. Moreover, to ease the adoption of MPC, industrial and academic MPC compilers have been developed to automatically translate programs describing arbitrary MPC procedures into low-level MPC executables. \\n Compiling high-level descriptions into high-efficiency MPC executables is challenging: the compilation often involves converting high-level languages into several intermediate representations (IR), e.g., arithmetic or boolean circuits, optimizing the computation/communication cost, and picking proper MPC protocols (and underlying virtual machines) for a particular task and threat model. Various optimizations and heuristics are employed during the compilation procedure to improve the efficiency of the generated MPC executables. \\n Despite the prosperous adoption of MPC compilers by industrial vendors and academia, a principled and systematic understanding of the correctness of MPC compilers does not yet exist. To fill this critical gap, this paper introduces MT-MPC, a metamorphic testing (MT) framework specifically designed for MPC compilers to effectively uncover erroneous compilations. Our approach proposes three metamorphic relations (MRs) that are tailored for MPC programs to mutate high-level MPC programs (compiler inputs). We then examine if MPC compilers yield semantics-equivalent MPC executables regarding the original and mutated MPC programs by comparing their execution results. \\n Real-world MPC compilers exhibit a high level of engineering quality. Nevertheless, we detected 4,772 inputs that can result in erroneous compilations in three popular MPC compilers available on the market. While the discovered error-triggering inputs do not cause the MPC compilers to crash directly, they can lead to the generation of incorrect MPC executables, jeopardizing the underlying dependability of the computation. With substantial manual effort and help from the MPC compiler developers, we uncovered thirteen bugs in these MPC compilers by debugging them using the error-triggering inputs. Our proposed testing frameworks and findings can be used to guide developers in their efforts to improve MPC compilers.\",\r\n        author: \"Yichen Li, Dongwei Xiao, Zhibo Liu, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/10.1145/3643781\",\r\n    },\r\n    {\r\n        id: 30,\r\n        title: \"Testing Graph Database Systems via Graph-Aware Metamorphic Relations\",\r\n        conf: \"VLDB\",\r\n        date: \"2024\",\r\n        abstract: \"Graph database systems (GDBs) have supported many important real-world applications such as social networks, logistics, and path planning. Meanwhile, logic bugs are also prevalent in GDBs, leading to incorrect results and severe consequences. However, the logic bugs largely cannot be revealed by prior solutions which are unaware of the graph native structures of the graph data. In this paper, we propose Gamera (Graph-aware metamorphic relations), a novel metamorphic testing approach to uncover unknown logic bugs in GDBs. We design three classes of novel graph-aware Metamorphic Relations (MRs) based on the graph native structures. Gamera would generate a set of queries according to the graph-aware MRs to test diverse and complex GDB operations, and check whether the GDB query results conform to the chosen MRs. \\n We thoroughly evaluated the effectiveness of Gamera on seven widely-used GDBs such as Neo4j and OrientDB. Gamera was highly effective in detecting logic bugs in GDBs. In total, it detected 39 logic bugs, of which 15 bugs have been confirmed, and three bugs have been fixed. Our experiments also demonstrated that Gamera significantly outperformed prior solutions including Grand, GD-smith and GDBMeter. Gamera has been well-recognized by GDB developers and we open-source our prototype implementation to contribute to the community.\",\r\n        author: \"Zeyang Zhuang, Penghui Li, Pingchuan Ma, Wei Meng, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/10.14778/3636218.3636236\",\r\n    },\r\n    {\r\n        id: 29,\r\n        title: \"On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study.\",\r\n        conf: \"ICSE\",\r\n        date: \"2024\",\r\n        abstract: 'Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. \\n In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \"code synthesis\" and \"code translation.\" We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.',\r\n        author: \"Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao, Yang Liu.\",\r\n        link: \"https://dl.acm.org/doi/abs/10.1145/3597503.3639091\",\r\n    },\r\n    {\r\n        id: 28,\r\n        title: \"Enabling Runtime Verification of Causal Discovery Algorithms with Automated Conditional Independence Reasoning.\",\r\n        conf: \"ICSE\",\r\n        date: \"2024\",\r\n        abstract: \"Causal discovery is a powerful technique for identifying causal relationships among variables in data. It has been widely used in various applications in software engineering. Causal discovery extensively involves conditional independence (CI) tests. Hence, its output quality highly depends on the performance of CI tests, which can often be unreliable in practice. Moreover, privacy concerns arise when excessive CI tests are performed. \\n Despite the distinct nature between unreliable and excessive CI tests, this paper identifies a unified and principled approach to addressing both of them. Generally, CI statements, the outputs of CI tests, adhere to Pearl's axioms, which are a set of well-established integrity constraints on conditional independence. Hence, we can either detect erroneous CI statements if they violate Pearl's axioms or prune excessive CI statements if they are logically entailed by Pearl's axioms. Holistically, both problems boil down to reasoning about the consistency of CI statements under Pearl's axioms (referred to as CIR problem). \\n We propose a runtime verification tool called CICheck, designed to harden causal discovery algorithms from reliability and privacy perspectives. CICheck employs a sound and decidable encoding scheme that translates CIR into SMT problems. To solve the CIR problem efficiently, CICheck introduces a four-stage decision procedure with three lightweight optimizations that actively prove or refute consistency, and only resort to costly SMT-based reasoning when necessary. Based on the decision procedure to CIR, CICheck includes two variants: ED-Check and P-Check, which detect erroneous CI tests (to enhance reliability) and prune excessive CI tests (to enhance privacy), respectively. We evaluate CICheck on four real-world datasets and 100 CIR instances, showing its effectiveness in detecting erroneous CI tests and reducing excessive CI tests while retaining practical performance.\",\r\n        author: \"Pingchuan Ma, Zhenlan Ji, Peisen Yao, Shuai Wang, and Kui Ren.\",\r\n        link: \"https://dl.acm.org/doi/10.1145/3597503.3623348\",\r\n    },\r\n    {\r\n        id: 27,\r\n        title: \"MPCDiff: Testing and Repairing MPC-Hardened Deep Learning Models.\",\r\n        conf: \"NDSS\",\r\n        date: \"2024\",\r\n        abstract: \"Secure multi-party computation (MPC) has recently become prominent as a concept to enable multiple parties to perform privacy-preserving machine learning without leaking sensitive data or details of pre-trained models to the other parties. Industry and the community have been actively developing and promoting high-quality MPC frameworks (e.g., based on TensorFlow and PyTorch) to enable the usage of MPC-hardened models, greatly easing the development cycle of integrating deep learning models with MPC primitives. \\n Despite the prosperous development and adoption of MPC frameworks, a principled and systematic understanding toward the correctness of those MPC frameworks does not yet exist. To fill this critical gap, this paper introduces MPCDiff, a differential testing framework to effectively uncover inputs that cause deviant outputs of MPC-hardened models and their plaintext versions. We further develop techniques to localize error-causing computation units in MPC-hardened models and automatically repair those defects. \\n We evaluate MPCDiff using real-world popular MPC frameworks for deep learning developed by Meta (Facebook), Alibaba Group, Cape Privacy, and OpenMined. MPCDiff successfully detected over one thousand inputs that result in largely deviant outputs. These deviation-triggering inputs are (visually) meaningful in comparison to regular inputs, indicating that our findings may cause great confusion in the daily usage of MPC frameworks. After localizing and repairing error-causing computation units, the robustness of MPC-hardened models can be notably enhanced without sacrificing accuracy and with negligible overhead.\",\r\n        author: \"Qi Pang, Yuanyuan Yuan, and Shuai Wang.\",\r\n        link: \"https://www.ndss-symposium.org/ndss-paper/mpcdiff-testing-and-repairing-mpc-hardened-deep-learning-models/\",\r\n    },\r\n    {\r\n        id: 26,\r\n        title: \"Explain Any Concept: Segment Anything Meets Concept-Based Explanation.\",\r\n        conf: \"NeurIPS\",\r\n        date: \"2023\",\r\n        abstract: \"EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas “concepts” in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any Concept (EAC), which explains DNN decisions with any concept. While SAM is highly effective and offers an “out-of-the-box” instance segmentation, it is costly when being integrated into defacto XAI pipelines. We thus propose a lightweight per-input equivalent (PIE) scheme, enabling efficient explanation with a surrogate model. Our evaluation over two popular datasets (ImageNet and COCO) illustrate the highly encouraging performance of EAC over commonly-used XAI methods.\",\r\n        author: \"Ao Sun, Pingchuan Ma, Yuanyuan Yuan, and Shuai Wang.\",\r\n        link: \"https://neurips.cc/virtual/2023/poster/71378\",\r\n    },\r\n    {\r\n        id: 25,\r\n        title: \"Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks.\",\r\n        conf: \"CCS\",\r\n        date: \"2023\",\r\n        abstract: 'The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. \\n We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.',\r\n        author: \"Zongjie Li, Chaozheng Wang, Shuai Wang, and Cuiyun Gao.\",\r\n        link: \"https://dl.acm.org/doi/10.1145/3576915.3623120\",\r\n    },\r\n    {\r\n        id: 24,\r\n        title: \"Towards Practical Federated Causal Structure Learning.\",\r\n        conf: \"ECML-PKDD\",\r\n        date: \"2023\",\r\n        abstract: \"Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of for causal structure learning in causal sufficiency and causal insufficiency, respectively. The study evaluates using both synthetic datasets and real-world data against existing solutions and finds it demonstrates encouraging performance and strong resilience to data heterogeneity among clients.\",\r\n        author: \"Zhaoyu Wang, Pingchuan Ma, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/10.1007/978-3-031-43415-0_21\",\r\n    },\r\n    {\r\n        id: 23,\r\n        title: \"Precise and Generalized Robustness Certification for Neural Networks.\",\r\n        conf: \"USENIX Security\",\r\n        date: \"2023\",\r\n        abstract: \"The objective of neural network (NN) robustness certification is to determine if a NN changes its predictions when mutations are made to its inputs. While most certification research studies pixel-level or a few geometrical-level and blurring operations over images, this paper proposes a novel framework, GCERT, which certifies NN robustness under a precise and unified form of diverse semantic-level image mutations. We formulate a comprehensive set of semantic-level image mutations uniformly as certain directions in the latent space of generative models. We identify two key properties, independence and continuity, that convert the latent space into a precise and analysis-friendly input space representation for certification. GCERT can be smoothly integrated with de facto complete, incomplete, or quantitative certification frameworks. With its precise input space representation, GCERT enables for the first time complete NN robustness certification with moderate cost under diverse semantic-level input mutations, such as weather-filter, style transfer, and perceptual changes (e.g., opening/closing eyes). We show that GCERT enables certifying NN robustness under various common and security-sensitive scenarios like autonomous driving.\",\r\n        author: \"Yuanyuan Yuan, Shuai Wang, and Zhendong Su.\",\r\n        link: \"https://dl.acm.org/doi/10.5555/3620237.3620504\",\r\n    },\r\n    {\r\n        id: 22,\r\n        title: \"BTD: Unleashing the Power of Decompilation for x86 Deep Neural Network Executables.\",\r\n        conf: \"Blackhat USA\",\r\n        date: \"2023\",\r\n        abstract: \"This course focuses on the decompilation of deep neural network (DNN) executables specifically for x86 architecture, aiming to enable learners to understand and reverse engineer compiled deep learning models. Participants will learn about the BTD (Bin to DNN) decompiler tool, which facilitates the analysis of DNN executables that operate across diverse computing platforms such as CPUs, GPUs, and hardware accelerators. The teaching method involves a detailed presentation that covers both theoretical aspects and practical applications of DNN decompilation. This course is intended for cybersecurity professionals, reverse engineers, and researchers interested in deep learning security and the inner workings of DNN executables.\",\r\n        author: \"Zhibo Liu, Yuanyuan Yuan, Xiaofei Xie, Tianxiang Li, Wenqiang Li, and Shuai Wang.\",\r\n        link: \"https://www.classcentral.com/course/youtube-btd-unleashing-the-power-of-decompilation-for-x86-deep-neural-network-executables-282986\",\r\n    },\r\n    {\r\n        id: 21,\r\n        title: \"Secure Federated Correlation Test and Entropy Estimation.\",\r\n        conf: \"ICML\",\r\n        date: \"2023\",\r\n        abstract: \"We propose the first federated correlation test framework compatible with secure aggregation, namely FED-χ2. In our protocol, the statistical computations are recast as frequency moment estimation problems, where the clients collaboratively generate a shared projection matrix and then use stable projection to encode the local information in a compact vector. As such encodings can be linearly aggregated, secure aggregation can be applied to conceal the individual updates. We formally establish the security guarantee of FED-χ2 by proving that only the minimum necessary information (i.e., the correlation statistics) is revealed to the server. We show that our protocol can be naturally extended to estimate other statistics that can be recast as frequency moment estimations. By accommodating Shannon'e Entropy in FED-χ2, we further propose the first secure federated entropy estimation protocol, FED-H. The evaluation results demonstrate that FED-χ2 and FED-H achieve good performance with small client-side computation overhead in several real-world case studies.\",\r\n        author: \"Qi Pang, Lun Wang, Shuai Wang, Wenting Zheng, and Dawn Song.\",\r\n        link: \"https://dl.acm.org/doi/10.5555/3618408.3619532\",\r\n    },\r\n    {\r\n        id: 20,\r\n        title: \"ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems.\",\r\n        conf: \"IEEE Security & Privacy\",\r\n        date: \"2023\",\r\n        abstract: \"Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the saliency score of ``victim'' participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.\",\r\n        author: \"Qi Pang, Yuanyuan Yuan, Shuai Wang, and Wenting Zheng.\",\r\n        link: \"https://arxiv.org/abs/2201.02775\",\r\n    },\r\n    {\r\n        id: 19,\r\n        title: \"XInsight: eXplainable Data Analysis Through The Lens of Causality.\",\r\n        conf: \"SIGMOD\",\r\n        date: \"2023\",\r\n        abstract: \"In light of the growing popularity of Exploratory Data Analysis (EDA), understanding the underlying causes of the knowledge acquired by EDA is crucial. However, it remains under-researched. This study promotes a transparent and explicable perspective on data analysis, called eXplainable Data Analysis (XDA). For this reason, we present XInsight, a general framework for XDA. XInsight provides data analysis with qualitative and quantitative explanations of causal and non-causal semantics. This way, it will significantly improve human understanding and confidence in the outcomes of data analysis, facilitating accurate data interpretation and decision making in the real world. XInsight is a three-module, end-to-end pipeline designed to extract causal graphs, translate causal primitives into XDA semantics, and quantify the quantitative contribution of each explanation to a data fact. XInsight uses a set of design concepts and optimizations to address the inherent difficulties associated with integrating causality into XDA. Experiments on synthetic and real-world datasets as well as a user study demonstrate the highly promising capabilities of XInsight.\",\r\n        author: \"Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, and Dongmei Zhang.\",\r\n        link: \"https://dl.acm.org/doi/10.1145/3589301\",\r\n    },\r\n    {\r\n        id: 18,\r\n        title: \"Byzantine-Robust Federated Learning with Optimal Statistical Rates.\",\r\n        conf: \"AISTATS\",\r\n        date: \"2023\",\r\n        abstract: \"Federated learning (FL) is vulnerable to Byzantine attacks due to its distributed nature. Existing defenses, which typically rely on server-based or trust-bootstrapped aggregation rules, often struggle to mitigate the impact when a large proportion of participants are malicious. Additionally, the absence of an effective incentive mechanism in current defenses may lead rational clients to submit meaningless or malicious updates, compromising the global model’s effectiveness in federated learning. To tackle these issues, we propose a Byzantine-robust Ensemble Incentive Mechanism (BEIM) that not only leverages ensemble learning to train multiple global models, enhancing the robustness against such attacks, but also establishes a novel incentive mechanism to promote honest participation. Specifically, Byzantine robustness of BEIM can be initially enhanced by motivating high-quality clients to participate in FL. A distance-based aggregation rule is then employed to diminish the influence of malicious clients. Subsequently, the integration of a majority voting scheme across the ensemble models further isolates and dilutes the impact of malicious updates. The properties of truthfulness, individual rationality, and budget feasibility of the incentive mechanism in BEIM are proved theoretically. Empirical results demonstrate that BEIM not only effectively counters the impact of malicious clients, enhancing test accuracy by up to 77.3% compared to existing baselines when over 50% of the clients are malicious, but also fairly rewards clients based on the quality of their contributions.\",\r\n        author: \"Banghua Zhu, Lun Wang, Qi Pang, Shuai Wang, Jiantao Jiao, Dawn Song, and Michael Jordan.\",\r\n        link: \"https://dl.acm.org/doi/10.1016/j.future.2024.05.017\",\r\n    },\r\n    {\r\n        id: 17,\r\n        title: \"OBSan: An Out-Of-Bound Sanitizer to Harden DNN Executables.\",\r\n        conf: \"NDSS\",\r\n        date: \"2023\",\r\n        abstract: \"The rapid adoption of deep neural network (DNN) models on a variety of hardware platforms has boosted the development of deep learning (DL) compilers. DL compilers take as input the high-level DNN model specifications and generate optimized DNN executables for diverse hardware architectures like CPUs and GPUs. Despite the emerging adoption of DL compilers in real-world scenarios, no solutions exist to protect DNN executables. To fill this critical gap, this paper introduces OBSAN, a fast sanitizer designed to check out-of-bound (OOB) behavior of DNN executables. From a holistic view, DNN incorporates bidirectional computation: forward propagation that predicts an output based on an input, and backward propagation that characterizes how the forward prediction is made. Both neuron activations in forward propagation and the gradients in backward propagation should fall within valid ranges, and deviations from the valid ranges would be considered as OOB. \\n OOB is primarily related to unsafe behavior of DNNs, which root from anomalous inputs and may cause mispredictions or even exploitation via adversarial examples (AEs). We thus design OBSAN, which includes two variants, FOBSAN and BOBSAN, that can detect OOB in the forward and backward propagations, respectively. Each OBSAN is designed as extra passes of DL compilers to integrate with large-scale DNN models, and we design various optimization schemes to reduce the overhead of OBSAN. Evaluations over various anomalous inputs show that OBSAN manifests promising OOB detectability with low overhead. We further present two downstream applications to show how OBSAN prevents online AE generation and facilitates feedback-driven fuzz testing toward DNN executables.\",\r\n        author: \"Yanzuo Chen, Yuanyuan Yuan, and Shuai Wang.\",\r\n        link: \"https://www.ndss-symposium.org/ndss-paper/obsan-an-out-of-bound-sanitizer-to-harden-dnn-executables/\",\r\n    },\r\n    {\r\n        id: 16,\r\n        title: \"Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion.\",\r\n        conf: \"ICSE\",\r\n        date: \"2023\",\r\n        abstract: \"Various deep neural network (DNN) coverage criteria have been proposed to assess DNN test inputs and steer input mutations. The coverage is characterized via neurons having certain outputs, or the discrepancy between neuron outputs. Nevertheless, recent research indicates that neuron coverage criteria show little correlation with test suite quality. In general, DNNs approximate distributions, by incorporating hierarchical layers, to make predictions for inputs. Thus, we champion to deduce DNN behaviors based on its approximated distributions from a layer perspective. A test suite should be assessed using its induced layer output distributions. Accordingly, to fully examine DNN behaviors, input mutation should be directed toward diversifying the approximated distributions. This paper summarizes eight design requirements for DNN coverage criteria, taking into account distribution properties and practical concerns. We then propose a new criterion, Neural Coverage (nlc),that satisfies all design requirements. NLC treats a single DNN layer as the basic computational unit (rather than a single neuron) and captures four critical properties of neuron output distributions. Thus, NL C accurately describes how DNNs comprehend inputs via approximated distributions. We demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks (classification and generation) and data formats (image and text). Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.\",\r\n        author: \"Yuanyuan Yuan, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://ieeexplore.ieee.org/document/10172683\",\r\n    },\r\n    {\r\n        id: 15,\r\n        title: \"CC: Causality-Aware Coverage Criterion for Deep Neural Networks.\",\r\n        conf: \"ICSE\",\r\n        date: \"2023\",\r\n        abstract: \"Deep neural network (DNN) testing approaches have grown fast in recent years to test the correctness and robustness of DNNs. In particular, DNN coverage criteria are frequently used to evaluate the quality of a test suite, and a number of coverage criteria based on neuron-wise, layer-wise, and path-/trace-wise coverage patterns have been published to date. However, we see that existing criteria are insufficient to represent how one neuron would influence subsequent neurons; hence, we lack a concept of how neurons, when functioning as causes and effects, might jointly make a DNN prediction. \\n Given recent advances in interpreting DNN internals using causal inference, we present the first causality-aware DNN coverage criterion, which evaluates a test suite by quantifying the extent to which the suite provides new causal relations for testing DNNs. Performing standard causal inference on DNNs presents both theoretical and practical hurdles. We introduce CC (causal coverage), a practical and efficient coverage criterion that integrates a set of optimizations using DNN domain-specific knowledge. We illustrate the efficacy of CC using diverse, real-world inputs and adversarial inputs, such as adversarial examples (AEs) and backdoor inputs. We demonstrate that CC outperforms previous DNN criteria under various settings with moderate cost.\",\r\n        author: \"Zhenlan Ji, Pingchuan Ma, Yuanyuan Yuan, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/10.1109/ICSE48619.2023.00153\",\r\n    },\r\n    {\r\n        id: 14,\r\n        title: \"CCTEST: Testing and Repairing Code Completion Systems.\",\r\n        conf: \"ICSE\",\r\n        date: \"2023\",\r\n        abstract: \"Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTEST features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the “average” appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity.\",\r\n        author: \"Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Dong Chen, Shuai Wang, and Cuiyun Gao.\",\r\n        link: \"https://ieeexplore.ieee.org/document/10172845\",\r\n    },\r\n    {\r\n        id: 13,\r\n        title: \"CacheQL: Quantifying and Localizing Cache Side-Channel Vulnerabilities in Production Software.\",\r\n        conf: \"USENIX Security\",\r\n        date: \"2023\",\r\n        abstract: \"Cache side-channel attacks extract secrets by examining how victim software accesses cache. To date, practical attacks on crypto systems and media libraries are demonstrated under different scenarios, inferring secret keys from crypto algorithms and reconstructing private media data such as images. \\n This work first presents eight criteria for designing a fullfledged detector for cache side-channel vulnerabilities. Then, we propose CacheQL, a novel detector that meets all of these criteria. CacheQL precisely quantifies information leaks of binary code, by characterizing the distinguishability of logged side channel traces. Moreover, CacheQL models leakage as a cooperative game, allowing information leakage to be precisely distributed to program points vulnerable to cache side channels. CacheQL is meticulously optimized to analyze whole side channel traces logged from production software (where each trace can have millions of records), and it alleviates randomness introduced by crypto blinding, ORAM, or real-world noises. \\n Our evaluation quantifies side-channel leaks of production crypto and media software. We further localize vulnerabilities reported by previous detectors and also identify a few hundred new vulnerable program points in recent OpenSSL (ver. 3.0.0), MbedTLS (ver. 3.0.0), Libgcrypt (ver. 1.9.4). Many of our localized program points are within the pre-processing modules of crypto libraries, which are not analyzed by existing works due to scalability. We also localize vulnerabilities in Libjpeg (ver. 2.1.2) that leak privacy about input images.\",\r\n        author: \"Yuanyuan Yuan, Zhibo Liu, and Shuai Wang.\",\r\n        link: \"https://www.usenix.org/conference/usenixsecurity23/presentation/yuan-yuanyuan-cacheql\",\r\n    },\r\n    {\r\n        id: 12,\r\n        title: \"Decompiling x86 Deep Neural Network Executables.\",\r\n        conf: \"USENIX Security\",\r\n        date: \"2023\",\r\n        abstract: \"Due to their widespread use on heterogeneous hardware devices, deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms, including CPUs, GPUs, and various hardware accelerators. \\n We present BTD (Bin to DNN), a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process DNN executables compiled by different DL compilers and with full optimizations enabled on x86 platforms. It employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators. \\n Our evaluation reveals that BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recovered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable. We show that BTD can boost two representative attacks, adversarial example generation and knowledge stealing, against DNN executables. We also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.\",\r\n        author: \"Zhibo Liu, Yuanyuan Yuan, Shuai Wang, Xiaofei Xie, and Lei Ma.\",\r\n        link: \"https://www.usenix.org/conference/usenixsecurity23/presentation/liu-zhibo\",\r\n    },\r\n    {\r\n        id: 11,\r\n        title: \"Unveiling the Hidden Defection of DNN Testing with Decision-Based Metamorphic Oracle.\",\r\n        conf: \"ASE\",\r\n        date: \"2022\",\r\n        abstract: \"Contemporary DNN testing works are frequently conducted using metamorphic testing (MT). In general, de facto MT frameworks mutate DNN input images using semantics-preserving mutations and determine if DNNs can yield consistent predictions. Nevertheless, we find that DNNs may rely on erroneous decisions (certain components on the DNN inputs) to make predictions, which may still retain the outputs by chance. Such DNN defects would be neglected by existing MT frameworks. Erroneous decisions, however, would likely result in successive mis-predictions over diverse images that may exist in real-life scenarios. \\n This research aims to unveil the pervasiveness of hidden DNN defects caused by incorrect DNN decisions (but retaining consistent DNN predictions). To do so, we tailor and optimize modern eXplainable AI (XAI) techniques to identify visual concepts that represent regions in an input image upon which the DNN makes predictions. Then, we extend existing MT-based DNN testing frameworks to check the consistency of DNN decisions made over a test input and its mutated inputs. Our evaluation shows that existing MT frameworks are oblivious to a considerable number of DNN defects caused by erroneous decisions. We conduct human evaluations to justify the validity of our findings and to elucidate their characteristics. Through the lens of DNN decision-based metamorphic relations, we re-examine the effectiveness of metamorphic transformations proposed by existing MT frameworks. We summarize lessons from this study, which can provide insights and guidelines for future DNN testing.\",\r\n        author: \"Yuanyuan Yuan, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://dlnext.acm.org/doi/abs/10.1145/3551349.3561157\",\r\n    },\r\n    {\r\n        id: 10,\r\n        title: \"NOLEAKS: Differentially Private Causal Discovery Under Functional Causal Model.\",\r\n        conf: \"IEEE TIFS\",\r\n        date: \"2022\",\r\n        abstract: \"Causal inference is widely used in clinical research, economic analysis, and other fields. As is the case with many statistical data, the findings of causal discovery (i.e., causal graph) might leak demographic information of participants. For example, a causal link between one genome and a rare disease can reveal the participation of a minority patient in genome-wide association studies. To date, differential privacy has served as the de facto foundation for guaranteeing the privacy of causal discovery algorithms. However, existing approaches to protecting causal discovery from privacy leakage rely heavily on private conditional independence tests, which generate a considerable amount of noise and are thus prone to inaccuracy. As a result of their limited accuracy and scalability, they are insufficient for non-trivial datasets (e.g., those with more than ten variables). In this paper, we advocate a novel focus on enforcing privacy for causal discovery algorithms based on functional causal models. First, we propose NoLeaks, a differentially private causal discovery algorithm, which manifests both high accuracy and efficiency compared with prior works. Second, we design a quasi-Newton numerical optimization algorithm for solving NoLeaks in a highly efficient way. Third, we evaluate NoLeaks using both public benchmarks and synthetic data. We observe that NoLeaks achieves comparable performance or even surpasses the state-of-the-art (non-private) approaches. We also find encouraging results that NoLeaks can smoothly scale to large datasets, on which existing works would fail. Through a case study and a downstream application, we observe encouraging results on the versatile usages of NoLeaks.\",\r\n        author: \"Pingchuan Ma, Zhenlan Ji, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://ieeexplore.ieee.org/document/9798874\",\r\n    },\r\n    {\r\n        id: 9,\r\n        title: \"MDPFuzz: Testing Models Solving Markov Decision Processes.\",\r\n        conf: \"ISSTA\",\r\n        date: \"2022\",\r\n        abstract: \"The Markov decision process (MDP) provides a mathematical frame- work for modeling sequential decision-making problems, many of which are crucial to security and safety, such as autonomous driving and robot control. The rapid development of artificial intelligence research has created efficient methods for solving MDPs, such as deep neural networks (DNNs), reinforcement learning (RL), and imitation learning (IL). However, these popular models solving MDPs are neither thoroughly tested nor rigorously reliable. \\n We present MDPFuzz, the first blackbox fuzz testing framework for models solving MDPs. MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states. During fuzzing, MDPFuzz decides which mutated state to retain by measuring if it can reduce cumulative rewards or form a new state sequence. We design efficient techniques to quantify the “freshness” of a state sequence using Gaussian mixture models (GMMs) and dynamic expectation-maximization (DynEM). We also prioritize states with high potential of revealing crashes by estimating the local sensitivity of target models over states. \\n MDPFuzz is evaluated on five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL. Our evaluation includes scenarios of autonomous driving, aircraft collision avoidance, and two games that are often used to benchmark RL. During a 12-hour run, we find over 80 crash-triggering state sequences on each model. We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states. We further develop an abnormal behavior detector to harden all the evaluated models and repair them with the findings of MDPFuzz to significantly enhance their robustness without sacrificing accuracy.\",\r\n        author: \"Qi Pang, Yuanyuan Yuan, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/abs/10.1145/3533767.3534388\",\r\n    },\r\n    {\r\n        id: 8,\r\n        title: \"Metamorphic Testing of Deep Learning Compilers.\",\r\n        conf: \"SIGMETRICS\",\r\n        date: \"2022\",\r\n        abstract: \"The prosperous trend of deploying deep neural network (DNN) models to diverse hardware platforms has boosted the development of deep learning (DL) compilers. DL compilers take high-level DNN model specifications as input and generate optimized DNN executables for diverse hardware architectures like CPUs, GPUs, and hardware accelerators. We introduce MT-DLComp, a metamorphic testing framework specifically designed for DL compilers to uncover erroneous compilations. Our approach leverages deliberately-designed metamorphic relations (MRs) to launch semantics-preserving mutations toward DNN models to generate their variants. This way, DL compilers can be automatically tested for compilation correctness by comparing the execution outputs of the compiled DNN models and their variants without manual intervention. We detected over 435 inputs that can result in erroneous compilations in four popular DL compilers, all of which are industry-strength products maintained by Amazon, Facebook, Microsoft, and Google. We uncovered four bugs in these compilers by debugging them using the error-triggering inputs.\",\r\n        author: \"Dongwei Xiao, Zhibo Liu, Yuanyuan Yuan, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/abs/10.1145/3547353.3522655?download=true\",\r\n    },\r\n    {\r\n        id: 7,\r\n        title: \"Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings.\",\r\n        conf: \"ICSE\",\r\n        date: \"2022\",\r\n        abstract: \"Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. \\n This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. \\n We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.\",\r\n        author: \"Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu.\",\r\n        link: \"https://dl.acm.org/doi/abs/10.1145/3510003.3510217\",\r\n    },\r\n    {\r\n        id: 6,\r\n        title: \"MT-Teql: Evaluating and Augmenting Neural NLIDB on Real-world Linguistic and Schema Variations.\",\r\n        conf: \"VLDB\",\r\n        date: \"2022\",\r\n        abstract: \"Natural Language Interface to Database (NLIDB) translates human utterances into SQL queries and enables database interactions for non-expert users. Recently, neural network models have become a major approach to implementing NLIDB. However, neural NLIDB faces challenges due to variations in natural language and database schema design. For instance, one user intent or database conceptual model can be expressed in various forms. However, existing benchmarks, using hold-out datasets, cannot provide thorough understanding of how good neural NLIDBs really are in real-world situations and its robustness against such variations. A key difficulty is to annotate SQL queries for inputs under real-world variations, requiring considerable manual effort and expert knowledge. \\n To systematically assess the robustness of neural NLIDBs without extensive manual effort, we propose MT-Teql, a unified framework to benchmark NLIDBs against real-world language and schema variations. Inspired by recent advances in DBMS metamorphic testing, MT-Teql implements semantics-preserving transformations on utterances and database schemas to generate their variants. NLIDBs can thus be examined for robustness utilizing utterances/schemas and their variants without requiring manual intervention. \\n We benchmarked nine neural NLIDBs using 62,430 inputs and identified 15,433 defects. We analyzed potential root causes of defects and conducted a user study to show how MT-Teql can assist developers to systematically assess NLIDBs. We further show that the transformed (error-triggering) inputs can be used to augment popular NLIDBs and eliminate 46.5%(±5.0%) errors made by them without compromising their accuracy on standard benchmarks. We summarize lessons from this study that can provide insights to select and design NLIDBs that fit particular usage scenarios.\",\r\n        author: \"Pingchuan Ma and Shuai Wang.\",\r\n        link: \"https://dl.acm.org/doi/10.14778/3494124.3494139\",\r\n    },\r\n    {\r\n        id: 5,\r\n        title: \"Automated Side Channel Analysis of Media Software with Manifold Learning.\",\r\n        conf: \"USENIX Security \",\r\n        date: \"2022\",\r\n        abstract: \"The prosperous development of cloud computing and machine learning as a service has led to the widespread use of media software to process confidential media data. This paper explores an adversary's ability to launch side channel analyses (SCA) against media software to reconstruct confidential media inputs. Recent advances in representation learning and perceptual learning inspired us to consider the reconstruction of media inputs from side channel traces as a cross-modality manifold learning task that can be addressed in a unified manner with an autoencoder framework trained to learn the mapping between media inputs and side channel observations. We further enhance the autoencoder with attention to localize the program points that make the primary contribution to SCA, thus automatically pinpointing information-leakage points in media software. \\n We also propose a novel and highly effective defensive technique called perception blinding that can perturb media inputs with perception masks and mitigate manifold learning-based SCA. Our evaluation exploits three popular media software to reconstruct inputs in image, audio, and text formats. We analyze three common side channels — cache bank, cache line, and page tables — and userspace-only cache set accesses logged by standard Prime+Probe. Our framework successfully re-constructs high-quality confidential inputs from the assessed media software and automatically pinpoint their vulnerable program points, many of which are unknown to the public. We further show that perception blinding can mitigate manifold learning-based SCA with negligible extra cost.\",\r\n        author: \"Yuanyuan Yuan, Qi Pang, and Shuai Wang.\",\r\n        link: \"https://www.usenix.org/conference/usenixsecurity22/presentation/yuan-yuanyuan\",\r\n    },\r\n    {\r\n        id: 4,\r\n        title: \"Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing.\",\r\n        conf: \"CVPR\",\r\n        date: \"2021\",\r\n        abstract: \"Visual question answering (VQA) takes an image and a natural-language question as input and returns a natural-language answer. To date, VQA models are primarily assessed by their accuracy on high-level reasoning questions. Nevertheless, Given that perception tasks (e.g., recognizing objects) are the building blocks in the compositional process required by high-level reasoning, there is a demanding need to gain insights into how much of a problem low-level perception is. Inspired by the principles of software metamorphic testing, we introduce MetaVQA, a model-agnostic framework for benchmarking perception capability of VQA models. Given an image i, MetaVQA is able to synthesize a low-level perception question q. It then jointly transforms (i, q) to one or a set of sub-questions and sub-images. MetaVQA checks whether the answer to (i, q) satisfies metamorphic relationships (MRs), denoting perception consistency, with the composed answers of transformed questions and images. Violating MRs denotes a failure of answering perception questions. MetaVQA successfully detects over 4.9 million perception failures made by popular VQA models with metamorphic testing. The state-of-the-art VQA models (e.g., the champion of VQA 2020 Challenge) suffer from perception consistency problems. In contrast, the Oscar VQA models, by using anchor points to align questions and images, show generally better consistency in perception tasks. We hope MetaVQA will revitalize interest in enhancing the low-level perceptual abilities of VQA models, a cornerstone of high-level reasoning.\",\r\n        author: \"Yuanyuan Yuan, Shuai Wang, Mingyue Jiang, and Tsong Yueh Chen.\",\r\n        link: \"https://ieeexplore.ieee.org/document/9578921\",\r\n    },\r\n    {\r\n        id: 3,\r\n        title: \"Private Image Reconstruction from System Side Channels Using Generative Models.\",\r\n        conf: \"ICLR\",\r\n        date: \"2021\",\r\n        abstract: \"System side channels denote effects imposed on the underlying system and hardware when running a program, such as its accessed CPU cache lines. Side channel analysis (SCA) allows attackers to infer program secrets based on observed side channel signals. Given the ever-growing adoption of machine learning as a service (MLaaS), image analysis software on cloud platforms has been exploited by reconstructing private user images from system side channels. Nevertheless, to date, SCA is still highly challenging, requiring technical knowledge of victim software's internal operations. For existing SCA attacks, comprehending such internal operations requires heavyweight program analysis or manual efforts. \\n This research proposes an attack framework to reconstruct private user images processed by media software via system side channels. The framework forms an effective workflow by incorporating convolutional networks, variational autoencoders, and generative adversarial networks. Our evaluation of two popular side channels shows that the reconstructed images consistently match user inputs, making privacy leakage attacks more practical. We also show surprising results that even one-bit data read/write pattern side channels, which are deemed minimally informative, can be used to reconstruct quality images using our framework.\",\r\n        author: \"Yuanyuan Yuan, Shuai Wang, and Junping Zhang.\",\r\n        link: \"https://openreview.net/forum?id=y06VOYLcQXa\",\r\n    },\r\n    {\r\n        id: 2,\r\n        title: \"Metamorphic Object Insertion for Testing Object Detection Systems.\",\r\n        conf: \"ASE\",\r\n        date: \"2020\",\r\n        abstract: \"Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs — similar to traditional software — may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist. To fill this critical gap, we introduce the design and realization of Metaod, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. Metaod is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, Metaod found tens of thousands of detection failures. To further demonstrate the practical usage of Metaod, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.\",\r\n        author: \"Shuai Wang and Zhendong Su.\",\r\n        link: \"https://ieeexplore.ieee.org/document/9286114\",\r\n    },\r\n    {\r\n        id: 1,\r\n        title: \"Metamorphic Testing and Certified Mitigation of Fairness Violations in NLP Models.\",\r\n        conf: \"IJCAI\",\r\n        date: \"2020\",\r\n        abstract: \"Natural language processing (NLP) models have been increasingly used in sensitive application domains including credit scoring, insurance, and loan assessment. Hence, it is critical to know that the decisions made by NLP models are free of unfair bias toward certain subpopulation groups. In this paper, we propose a novel framework employing metamorphic testing, a well-established software testing scheme, to test NLP models and find discriminatory inputs that provoke fairness violations. Furthermore, inspired by recent breakthroughs in the certified robustness of machine learning, we formulate NLP model fairness in a practical setting as (ε, k)-fairness and accordingly smooth the model predictions to mitigate fairness violations. We demonstrate our technique using popular (commercial) NLP models, and successfully flag thousands of discriminatory inputs that can cause fairness violations. We further enhance the evaluated models by adding certified fairness guarantee at a modest cost.\",\r\n        author: \"Pingchuan Ma, Shuai Wang, and Jin Liu. \",\r\n        link: \"https://www.ijcai.org/Proceedings/2020/64\",\r\n    },\r\n    // Add more articles as needed\r\n];"],"names":["articles","id","title","conf","date","abstract","author","link"],"sourceRoot":""}